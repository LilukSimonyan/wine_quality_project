{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Wine Quality using Wine Quality Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules that will be used in process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import cvxopt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from pandas.core.common import SettingWithCopyWarning\n",
    "# warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from numpy import mean, std,isnan, asarray, polyfit\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier #SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,recall_score, precision_score,classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data set (csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have got df_final dataframe in our Getting_best_dataframe file. So here we can import our save file.\n",
    "#### df_final = pd.read_csv('Dataframe_final.csv')\n",
    "#### df_final.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequalityN.csv\")\n",
    "# df.sample(5)                  #shows 5 random choosen rows\n",
    "# df.head(5)                    #shows first 5 rows from data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting nomalized data from given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_correcting(data_):\n",
    "    \n",
    "    \"\"\" In dataset some values for fixed acidity, volatile acidity,\n",
    "    citric acid, residual sugar, chlorides, pH, sulphates are missing. \n",
    "    Solve this problem by filling null values with mean values of train dataframe.\n",
    "     irst of all we need to change text values of wine type to the 0 and 1.\n",
    "    Then we need to get train data and Fill Nan values by this data mean.\"\"\"\n",
    "    \n",
    "    data_[\"type\"].replace({\"red\": 0, \"white\": 1}, inplace=True) # \"RED\": 0, \"WHITE\": 1       \n",
    "    train_df = data_.sample(frac=0.8, random_state=42)\n",
    "    df.fillna(train_df.mean(axis=0), inplace=True)\n",
    "    \n",
    "    \"\"\" Now we have 1599 red and 4898 white wine rows\n",
    "    You can check it by this:\n",
    "    dups_color = df.pivot_table(index=['type'], aggfunc='size')\n",
    "    dups_color \"\"\"\n",
    "    \n",
    "    return data_\n",
    "\n",
    "df = data_correcting(df)\n",
    "\n",
    "def oversampling_data(data_):\n",
    "    \n",
    "    \"\"\" Share of white wines is 75%. So we deside to oversampling data with random choosen red wine data.\n",
    "    As in the future we can have other share of white and red wines, we get this solution for all possible cases. \"\"\"\n",
    "    \n",
    "    red_count = df.loc[data_['type'] == 0].count()[0]\n",
    "    white_count = df.loc[data_['type'] == 1].count()[0]\n",
    "    if white_count > red_count:\n",
    "        for i in range((white_count-red_count)):\n",
    "            df1 = data_.loc[data_['type'] == 0].sample()\n",
    "            data_ = data_.append(df1)\n",
    "    else:\n",
    "        for i in range((red_count-white_count)):\n",
    "            df1 = data_.loc[df['type'] == 1].sample()\n",
    "            data_ = data_.append(df1)\n",
    "            \n",
    "    \"\"\" Now combining fixed acidity, volatile acidity and citric acid into one variable total_acidity\n",
    "    and our target variable into two classes: low quality-->0 (3, 4, 5)  and high quality-->1 (6,7,8,9)\"\"\"\n",
    "    \n",
    "    data_[\"total_acidity\"]= data_['fixed acidity']+data_['volatile acidity']+data_['citric acid']\n",
    "    quaity_mapping = { 3 : 0, 4 : 0, 5: 0, 6 : 1, 7: 1, 8 : 1, 9 : 1}\n",
    "    data_[\"quality\"] =  data_[\"quality\"].map(quaity_mapping)\n",
    "    \n",
    "    \"\"\"You can check that it works by this\n",
    "    dups_color = df.pivot_table(index=['type'], aggfunc='size')\n",
    "    dups_color\n",
    "    Now we have 4898 of red and 4898 of white wines data \"\"\"\n",
    "    \n",
    "    return data_\n",
    "    \n",
    "\n",
    "# From all EDA analyze we can see that there are some outliers. So we have 2 variants\n",
    "# 1. Remove this outliers.\n",
    "# 2. Replace them with max/min values, so they may contain good values for other features and this variant will save their values\n",
    "# The whole analyse we have shown in the initial stage of project. So here we will show only the result of our final decisioans.\n",
    "# 3. Combine our target variable into two classes: low quality (3, 4, 5)  and high quality (6,7,8,9)\n",
    "\n",
    "\n",
    "def first_data(data_):\n",
    "    lower_limit = data_[\"free sulfur dioxide\"].mean() - 3*data_[\"free sulfur dioxide\"].std()\n",
    "    upper_limit = data_[\"free sulfur dioxide\"].mean() + 3*data_[\"free sulfur dioxide\"].std()\n",
    "    df2 = data_[(data_[\"free sulfur dioxide\"] > lower_limit) & (data_[\"free sulfur dioxide\"] < upper_limit)]\n",
    "    lower_limit = df2['total sulfur dioxide'].mean() - 3*df2['total sulfur dioxide'].std()\n",
    "    upper_limit = df2['total sulfur dioxide'].mean() + 3*df2['total sulfur dioxide'].std()\n",
    "    df3 = df2[(df2['total sulfur dioxide'] > lower_limit) & (df2['total sulfur dioxide'] < upper_limit)]\n",
    "    lower_limit = df3['residual sugar'].mean() - 3*df3['residual sugar'].std()\n",
    "    upper_limit = df3['residual sugar'].mean() + 3*df3['residual sugar'].std()\n",
    "    df4 = df3[(df3['residual sugar'] > lower_limit) & (df3['residual sugar'] < upper_limit)]\n",
    "    \n",
    "    return df4\n",
    "    \n",
    "dataframe_1 = first_data(oversampling_data(df)).copy()\n",
    "\n",
    "def second_data(data_):\n",
    "    lower_limit = data_[\"free sulfur dioxide\"].mean() - 3*data_[\"free sulfur dioxide\"].std()\n",
    "    upper_limit = data_[\"free sulfur dioxide\"].mean() + 3*data_[\"free sulfur dioxide\"].std()\n",
    "    df2_repl = data_\n",
    "    \n",
    "    def replace_outliers(arr):\n",
    "        arr = np.array(arr)\n",
    "        upper = arr.mean() + 3 * arr.std()\n",
    "        lower = arr.mean() - 3 * arr.std()\n",
    "        arr[(arr > upper)] = upper\n",
    "        arr[(arr < lower)] = lower\n",
    "        \n",
    "        return arr\n",
    "    \n",
    "    df2_repl[\"free sulfur dioxide\"] = replace_outliers(df2_repl[\"free sulfur dioxide\"])\n",
    "    df2_repl[\"total sulfur dioxide\"] = replace_outliers(df2_repl[\"total sulfur dioxide\"])\n",
    "    df2_repl[\"residual sugar\"] = replace_outliers(df2_repl[\"residual sugar\"])\n",
    "\n",
    "    lower_limit = df2_repl[\"free sulfur dioxide\"].mean() - 3*df2_repl[\"free sulfur dioxide\"].std()\n",
    "    upper_limit = df2_repl[\"free sulfur dioxide\"].mean() + 3*df2_repl[\"free sulfur dioxide\"].std()\n",
    "    \n",
    "    return df2_repl\n",
    "\n",
    "dataframe_2 = second_data(oversampling_data(df)).copy()\n",
    "\n",
    "\n",
    "def lst_of_dataframes(d1, d2):\n",
    "    \n",
    "    \"\"\" list of dataframe_1 and dataframe_2 \"\"\"\n",
    "    \n",
    "    df_list = [d1, d2] \n",
    "    for i in range(len(df_list)):\n",
    "        df_ = df_list[i]\n",
    "        df_final = df_[[\"total_acidity\", \"chlorides\", \"pH\", \"sulphates\", \"alcohol\", \"quality\"]]\n",
    "        df_list[i] = df_final\n",
    "        \n",
    "    return df_list\n",
    "        \n",
    "def get_dataset(dataframe):\n",
    "    \n",
    "    X = dataframe.drop(\"quality\", axis = 1)\n",
    "    y = dataframe[\"quality\"]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def checking_better_dataframe(lst):\n",
    "    \n",
    "    #lst = lst_of_dataframes(dataframe_1, dataframe_2)\n",
    "    \n",
    "    \"\"\" This function will get accuracy for two dataframes that we have, compare them\n",
    "    and return the dataframe wich gives us better accuracy. \"\"\"\n",
    "    \n",
    "    l_accuracy = []\n",
    "    \n",
    "    for i in range(len(lst)):\n",
    "        X, y = get_dataset(lst[i])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "        log_reg = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                       intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                       multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                       random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                       warm_start=False)\n",
    "        log_reg.fit(X_train,y_train)\n",
    "        y_hat = log_reg.predict(X_test)        \n",
    "        cm = confusion_matrix(y_hat,y_test)\n",
    "        accuracy = metrics.accuracy_score(y_hat,y_test)\n",
    "        l_accuracy.append(accuracy)\n",
    "#         print(f'Accuracy of dataframe_{i+1} is {accuracy}')\n",
    "#         print(f'log_reg.intercept_ of dataframe_{i+1} is {log_reg.intercept_}')\n",
    "#         print(f'log_reg.coef_ of dataframe_{i+1} is {log_reg.coef_}')\n",
    "#         print(f'confusion_matrix of dataframe_{i+1} is {cm}\\n')\n",
    "    if l_accuracy[0] > l_accuracy[1]:\n",
    "        \n",
    "        return lst[0]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return lst[1]\n",
    "        \n",
    "df_final = checking_better_dataframe(lst_of_dataframes(dataframe_1, dataframe_2))[[\"total_acidity\", \"chlorides\", \"pH\", \"sulphates\", \"alcohol\", \"quality\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have 2 datasets for modeling\n",
    "#### 1.1) dataframe_1 (where outliers removed, quality scaled to 2 types)\n",
    "#### 1.2) dataframe_2 (where outliers replaced with mean values, quality scaled to 2 types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic regression\n",
    "# 2. Cross-validation - KFold\n",
    "# 3. RandomForestClassifier\n",
    "# 4. SVM\n",
    "# 5. Kernel SVM\n",
    "# 6. Gaussian Kernel\n",
    "# 7. Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset transformation function\n",
    "def get_dataset(dataframe):\n",
    "\n",
    "    X = dataframe.drop(\"quality\", axis = 1)\n",
    "    y = dataframe[\"quality\"]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def log_regression(dataframe):\n",
    "    X, y = get_dataset(dataframe)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    #Logistic regression\n",
    "    log_reg = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                       intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                       multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                       random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                       warm_start=False)\n",
    "    log_reg.fit(X_train,y_train)\n",
    "    y_hat = log_reg.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_hat,y_test)\n",
    "    cm = confusion_matrix(y_hat,y_test)\n",
    "    # Let's predict target values for test dataset and check model accuracy\n",
    "    print('Logistic regression')\n",
    "    print(f'Accuracy of dataframe is: {accuracy}')\n",
    "    print(f'log_reg_intercept of dataframe is: {log_reg.intercept_}')\n",
    "    print(f'log_reg_coef of dataframe is: {log_reg.coef_}')\n",
    "    print(f'confusion_matrix of dataframe is:\\n {cm}\\n')\n",
    "\n",
    "#Cross-validation - KFold\n",
    "def k_fold_model(dataframe, k):\n",
    "    X, y = get_dataset(dataframe)\n",
    "\n",
    "    # retrieve the model to be evaluate\n",
    "    def get_model():\n",
    "        model = LogisticRegression()\n",
    "        return model\n",
    "\n",
    "    # evaluate the model using a given test condition\n",
    "    def evaluate_model(cv):\n",
    "        # get the dataset\n",
    "        #X, y = fit_dataset(dataframe)\n",
    "        # get the model\n",
    "        model = get_model()\n",
    "        # evaluate the model\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        # return scores\n",
    "        return mean(scores), scores.min(), scores.max()\n",
    "\n",
    "\n",
    "    print('Cross-validation - KFold')\n",
    "    # define folds to test\n",
    "    folds = range(2, k+1)\n",
    "\n",
    "    # record mean and min/max of each set of results\n",
    "    means, mins, maxs = list(),list(),list()\n",
    "    # evaluate each k value\n",
    "    for k in folds:\n",
    "        # define the test condition\n",
    "        cv = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "        # evaluate k value\n",
    "        k_mean, k_min, k_max = evaluate_model(cv)\n",
    "        # report performance        \n",
    "        print('> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n",
    "        # store mean accuracy\n",
    "        means.append(k_mean)\n",
    "        # store min and max relative to the mean\n",
    "        mins.append(k_mean - k_min)\n",
    "        maxs.append(k_max - k_mean)\n",
    "    # calculate the ideal test condition\n",
    "    ideal, _, _ = evaluate_model(LeaveOneOut())\n",
    "    print()\n",
    "    print('Ideal: %.3f' % ideal)\n",
    "    print()\n",
    "\n",
    "#RandomForestClassifier\n",
    "def rand_forest(dataframe):\n",
    "    X, y = get_dataset(dataframe)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    rf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                           criterion='gini', max_depth=None, max_features='auto',\n",
    "                           max_leaf_nodes=None, max_samples=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                           n_jobs=None, oob_score=False, random_state=None,\n",
    "                           verbose=0, warm_start=False)\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred=rf.predict(X_test)\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    accuracy2 = metrics.accuracy_score(y_pred, y_test)\n",
    "    cm2 = confusion_matrix(y_pred,y_test)\n",
    "    print('RandomForestClassifier')\n",
    "    print(f'Accuracy of dataframe is: {accuracy2}')\n",
    "    print(f'confusion_matrix of dataframe is:\\n {cm2}\\n')\n",
    "\n",
    "#SVM Linear, Kernel, Gaussian Kernel, Sigmoid\n",
    "def svm_func(dataframe):\n",
    "    X, y = get_dataset(dataframe)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    kernel_hyperparameter = {'SVM':'linear', 'Kernel SVM':'poly', 'Gaussian Kernel SVM':'rbf',\n",
    "                             'Sigmoid Kernel SVM':'sigmoid'}\n",
    "    for key, value in kernel_hyperparameter.items():\n",
    "        svclassifier = SVC(kernel=value)\n",
    "        svclassifier.fit(X_train, y_train)\n",
    "        y_pred = svclassifier.predict(X_test)\n",
    "        print(f'{key}')\n",
    "        print(f'Accuracy of dataframe is: {metrics.accuracy_score(y_pred, y_test)}')\n",
    "        print(f'confusion_matrix of dataframe is:\\n {confusion_matrix(y_test,y_pred)}\\n')\n",
    "    #     print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "Accuracy of dataframe is: 0.7061224489795919\n",
      "log_reg_intercept of dataframe is: [0.52057434]\n",
      "log_reg_coef of dataframe is: [[-0.12599011 -0.43827177 -0.27608609  0.42406727  0.9966723 ]]\n",
      "confusion_matrix of dataframe is:\n",
      " [[489 298]\n",
      " [278 895]]\n",
      "\n",
      "Cross-validation - KFold\n",
      "> folds=2, accuracy=0.700 (0.697,0.703)\n",
      "> folds=3, accuracy=0.699 (0.694,0.705)\n",
      "> folds=4, accuracy=0.700 (0.693,0.705)\n",
      "> folds=5, accuracy=0.700 (0.693,0.712)\n",
      "> folds=6, accuracy=0.701 (0.691,0.707)\n",
      "> folds=7, accuracy=0.699 (0.689,0.707)\n",
      "> folds=8, accuracy=0.699 (0.682,0.711)\n",
      "> folds=9, accuracy=0.701 (0.688,0.714)\n",
      "> folds=10, accuracy=0.701 (0.688,0.713)\n",
      "> folds=11, accuracy=0.700 (0.683,0.726)\n",
      "> folds=12, accuracy=0.700 (0.670,0.727)\n",
      "> folds=13, accuracy=0.701 (0.678,0.720)\n",
      "> folds=14, accuracy=0.700 (0.684,0.720)\n",
      "> folds=15, accuracy=0.700 (0.668,0.720)\n",
      "> folds=16, accuracy=0.700 (0.652,0.725)\n",
      "> folds=17, accuracy=0.701 (0.669,0.726)\n",
      "> folds=18, accuracy=0.701 (0.677,0.726)\n",
      "> folds=19, accuracy=0.701 (0.651,0.728)\n",
      "> folds=20, accuracy=0.701 (0.657,0.724)\n",
      "> folds=21, accuracy=0.701 (0.665,0.740)\n",
      "> folds=22, accuracy=0.701 (0.665,0.746)\n",
      "> folds=23, accuracy=0.700 (0.643,0.739)\n",
      "> folds=24, accuracy=0.701 (0.669,0.745)\n",
      "> folds=25, accuracy=0.700 (0.661,0.729)\n",
      "> folds=26, accuracy=0.701 (0.647,0.735)\n",
      "> folds=27, accuracy=0.701 (0.647,0.733)\n",
      "> folds=28, accuracy=0.702 (0.657,0.734)\n",
      "> folds=29, accuracy=0.702 (0.663,0.746)\n",
      "> folds=30, accuracy=0.701 (0.645,0.742)\n",
      "\n",
      "Ideal: 0.701\n",
      "\n",
      "RandomForestClassifier\n",
      "Accuracy of dataframe is: 0.8770408163265306\n",
      "confusion_matrix of dataframe is:\n",
      " [[ 625   99]\n",
      " [ 142 1094]]\n",
      "\n",
      "SVM\n",
      "Accuracy of dataframe is: 0.7056122448979592\n",
      "confusion_matrix of dataframe is:\n",
      " [[533 234]\n",
      " [343 850]]\n",
      "\n",
      "Kernel SVM\n",
      "Accuracy of dataframe is: 0.688265306122449\n",
      "confusion_matrix of dataframe is:\n",
      " [[ 304  463]\n",
      " [ 148 1045]]\n",
      "\n",
      "Gaussian Kernel SVM\n",
      "Accuracy of dataframe is: 0.7362244897959184\n",
      "confusion_matrix of dataframe is:\n",
      " [[519 248]\n",
      " [269 924]]\n",
      "\n",
      "Sigmoid Kernel SVM\n",
      "Accuracy of dataframe is: 0.6030612244897959\n",
      "confusion_matrix of dataframe is:\n",
      " [[381 386]\n",
      " [392 801]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_regression(df_final)\n",
    "k_fold_model(df_final, 30) # k is the fold number, here k = 30\n",
    "rand_forest(df_final)\n",
    "svm_func(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We got higher accuracy score in case of RandomForest model, so we will choose this model for predicting wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the results of several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LogisticRegression: ideal=0.701, cv=0.701\n",
      ">RidgeClassifier: ideal=0.700, cv=0.700\n",
      ">KNeighborsClassifier: ideal=0.820, cv=0.811\n",
      ">DecisionTreeClassifier: ideal=0.876, cv=0.865\n",
      ">RandomForestClassifier: ideal=0.899, cv=0.890\n",
      ">LinearSVC: ideal=0.700, cv=0.700\n",
      ">SVC: ideal=0.735, cv=0.736\n",
      "Correlation: 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the dataset\n",
    "def get_dataset(df_final):\n",
    "    X = df_final.drop(\"quality\", axis = 1)\n",
    "    y = df_final[\"quality\"]\n",
    "    norm = StandardScaler().fit(X)\n",
    "    X = pd.DataFrame(columns = X.columns, data = norm.transform(X))\n",
    "    return X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(LogisticRegression())\n",
    "    models.append(RidgeClassifier())\n",
    "    models.append(KNeighborsClassifier())\n",
    "    models.append(DecisionTreeClassifier())\n",
    "    models.append(RandomForestClassifier())\n",
    "    models.append(LinearSVC())\n",
    "    models.append(SVC())        \n",
    "#     models.append(GaussianNB())\n",
    "#     models.append(ExtraTreesClassifier())\n",
    "#     models.append(BaggingClassifier())\n",
    "#     models.append(GaussianProcessClassifier())\n",
    "#     models.append(GradientBoostingClassifier())\n",
    "    return models\n",
    " \n",
    "# evaluate the model using a given test condition\n",
    "def evaluate_model(cv, model):\n",
    "    # get the dataset\n",
    "    X, y = get_dataset(df_final)\n",
    "    # evaluate the model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # return scores\n",
    "    return mean(scores)\n",
    " \n",
    "# define test conditions\n",
    "ideal_cv = LeaveOneOut()\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# get the list of models to consider\n",
    "models = get_models()\n",
    "# collect results\n",
    "ideal_results, cv_results = list(), list()\n",
    "# evaluate each model\n",
    "for model in models:\n",
    "    # evaluate model using each test condition\n",
    "    cv_mean = evaluate_model(cv, model)\n",
    "    ideal_mean = evaluate_model(ideal_cv, model)\n",
    "    # check for invalid results\n",
    "    if isnan(cv_mean) or isnan(ideal_mean):\n",
    "        continue\n",
    "    # store results\n",
    "    cv_results.append(cv_mean)\n",
    "    ideal_results.append(ideal_mean)\n",
    "    # summarize progress\n",
    "    print('>%s: ideal=%.3f, cv=%.3f' % (type(model).__name__, ideal_mean, cv_mean))\n",
    "# calculate the correlation between each test condition\n",
    "corr, _ = pearsonr(cv_results, ideal_results)\n",
    "print('Correlation: %.3f' % corr)\n",
    "# scatter plot of results\n",
    "plt.scatter(cv_results, ideal_results)\n",
    "# plot the line of best fit\n",
    "coeff, bias = polyfit(cv_results, ideal_results, 1)\n",
    "line = coeff * asarray(cv_results) + bias\n",
    "plt.plot(cv_results, line, color='r')\n",
    "# label the plot\n",
    "plt.title('10-fold CV vs LOOCV Mean Accuracy')\n",
    "plt.xlabel('Mean Accuracy (10-fold CV)')\n",
    "plt.ylabel('Mean Accuracy (LOOCV)')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataframe):\n",
    "\n",
    "    X = dataframe.drop(\"quality\", axis = 1)\n",
    "    y = dataframe[\"quality\"]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = get_dataset(df_final)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "rf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "rf.fit(X_train,y_train)\n",
    "# save the model to disk\n",
    "filename = 'instance.json'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
