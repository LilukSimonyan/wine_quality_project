{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Wine Quality using Wine Quality Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules that will be used in process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import cvxopt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from pandas.core.common import SettingWithCopyWarning\n",
    "# warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from numpy import mean, std,isnan, asarray, polyfit\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier #SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,recall_score, precision_score,classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data set (csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have got df_final dataframe in our Getting_best_dataframe file. So here we can import our save file.\n",
    "#### df_final = pd.read_csv('Dataframe_final.csv')\n",
    "#### df_final.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequalityN.csv\")\n",
    "# df.sample(5)                  #shows 5 random choosen rows\n",
    "# df.head(5)                    #shows first 5 rows from data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting normalized data from given dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_correcting(data_):\n",
    "    \n",
    "    \"\"\" In dataset some values for fixed acidity, volatile acidity,\n",
    "    citric acid, residual sugar, chlorides, pH, sulphates are missing. \n",
    "    Solve this problem by filling null values with mean values of train dataframe.\n",
    "     irst of all we need to change text values of wine type to the 0 and 1.\n",
    "    Then we need to get train data and Fill Nan values by this data mean.\"\"\"\n",
    "    \n",
    "    data_[\"type\"].replace({\"red\": 0, \"white\": 1}, inplace=True) # \"RED\": 0, \"WHITE\": 1       \n",
    "    train_df = data_.sample(frac=0.8, random_state=42)\n",
    "    df.fillna(train_df.mean(axis=0), inplace=True)\n",
    "    \n",
    "    \"\"\" Now we have 1599 red and 4898 white wine rows\n",
    "    You can check it by this:\n",
    "    dups_color = df.pivot_table(index=['type'], aggfunc='size')\n",
    "    dups_color \"\"\"\n",
    "    \n",
    "    return data_\n",
    "\n",
    "df = data_correcting(df)\n",
    "\n",
    "def oversampling_data(data_):\n",
    "    \n",
    "    \"\"\" Share of white wines is 75%. So we deside to oversampling data with random choosen red wine data.\n",
    "    As in the future we can have other share of white and red wines, we get this solution for all possible cases. \"\"\"\n",
    "    \n",
    "    red_count = df.loc[data_['type'] == 0].count()[0]\n",
    "    white_count = df.loc[data_['type'] == 1].count()[0]\n",
    "    if white_count > red_count:\n",
    "        for i in range((white_count-red_count)):\n",
    "            df1 = data_.loc[data_['type'] == 0].sample()\n",
    "            data_ = data_.append(df1)\n",
    "    else:\n",
    "        for i in range((red_count-white_count)):\n",
    "            df1 = data_.loc[df['type'] == 1].sample()\n",
    "            data_ = data_.append(df1)\n",
    "            \n",
    "    \"\"\" Now combining fixed acidity, volatile acidity and citric acid into one variable total_acidity\n",
    "    and our target variable into two classes: low quality-->0 (3, 4, 5)  and high quality-->1 (6,7,8,9)\"\"\"\n",
    "    \n",
    "    data_[\"total_acidity\"]= data_['fixed acidity']+data_['volatile acidity']+data_['citric acid']\n",
    "    quaity_mapping = { 3 : 0, 4 : 0, 5: 0, 6 : 1, 7: 1, 8 : 1, 9 : 1}\n",
    "    data_[\"quality\"] =  data_[\"quality\"].map(quaity_mapping)\n",
    "    \n",
    "    \"\"\"You can check that it works by this\n",
    "    dups_color = df.pivot_table(index=['type'], aggfunc='size')\n",
    "    dups_color\n",
    "    Now we have 4898 of red and 4898 of white wines data \"\"\"\n",
    "    \n",
    "    return data_\n",
    "    \n",
    "\n",
    "# From all EDA analyze we can see that there are some outliers. So we have 2 variants\n",
    "# 1. Remove this outliers.\n",
    "# 2. Replace them with max/min values, so they may contain good values for other features and this variant will save their values\n",
    "# The whole analyse we have shown in the initial stage of project. So here we will show only the result of our final decisioans.\n",
    "# 3. Combine our target variable into two classes: low quality (3, 4, 5)  and high quality (6,7,8,9)\n",
    "\n",
    "\n",
    "def first_data(data_):\n",
    "    lower_limit = data_[\"free sulfur dioxide\"].mean() - 3*data_[\"free sulfur dioxide\"].std()\n",
    "    upper_limit = data_[\"free sulfur dioxide\"].mean() + 3*data_[\"free sulfur dioxide\"].std()\n",
    "    df2 = data_[(data_[\"free sulfur dioxide\"] > lower_limit) & (data_[\"free sulfur dioxide\"] < upper_limit)]\n",
    "    lower_limit = df2['total sulfur dioxide'].mean() - 3*df2['total sulfur dioxide'].std()\n",
    "    upper_limit = df2['total sulfur dioxide'].mean() + 3*df2['total sulfur dioxide'].std()\n",
    "    df3 = df2[(df2['total sulfur dioxide'] > lower_limit) & (df2['total sulfur dioxide'] < upper_limit)]\n",
    "    lower_limit = df3['residual sugar'].mean() - 3*df3['residual sugar'].std()\n",
    "    upper_limit = df3['residual sugar'].mean() + 3*df3['residual sugar'].std()\n",
    "    df4 = df3[(df3['residual sugar'] > lower_limit) & (df3['residual sugar'] < upper_limit)]\n",
    "    \n",
    "    return df4\n",
    "    \n",
    "dataframe_1 = first_data(oversampling_data(df)).copy()\n",
    "\n",
    "def second_data(data_):\n",
    "    lower_limit = data_[\"free sulfur dioxide\"].mean() - 3*data_[\"free sulfur dioxide\"].std()\n",
    "    upper_limit = data_[\"free sulfur dioxide\"].mean() + 3*data_[\"free sulfur dioxide\"].std()\n",
    "    df2_repl = data_\n",
    "    \n",
    "    def replace_outliers(arr):\n",
    "        arr = np.array(arr)\n",
    "        upper = arr.mean() + 3 * arr.std()\n",
    "        lower = arr.mean() - 3 * arr.std()\n",
    "        arr[(arr > upper)] = upper\n",
    "        arr[(arr < lower)] = lower\n",
    "        \n",
    "        return arr\n",
    "    \n",
    "    df2_repl[\"free sulfur dioxide\"] = replace_outliers(df2_repl[\"free sulfur dioxide\"])\n",
    "    df2_repl[\"total sulfur dioxide\"] = replace_outliers(df2_repl[\"total sulfur dioxide\"])\n",
    "    df2_repl[\"residual sugar\"] = replace_outliers(df2_repl[\"residual sugar\"])\n",
    "\n",
    "    lower_limit = df2_repl[\"free sulfur dioxide\"].mean() - 3*df2_repl[\"free sulfur dioxide\"].std()\n",
    "    upper_limit = df2_repl[\"free sulfur dioxide\"].mean() + 3*df2_repl[\"free sulfur dioxide\"].std()\n",
    "    \n",
    "    return df2_repl\n",
    "\n",
    "dataframe_2 = second_data(oversampling_data(df)).copy()\n",
    "\n",
    "\n",
    "def lst_of_dataframes(d1, d2):\n",
    "    \n",
    "    \"\"\" list of dataframe_1 and dataframe_2 \"\"\"\n",
    "    \n",
    "    df_list = [d1, d2] \n",
    "    for i in range(len(df_list)):\n",
    "        df_ = df_list[i]\n",
    "        df_final = df_[[\"total_acidity\", \"chlorides\", \"pH\", \"sulphates\", \"alcohol\", \"quality\"]]\n",
    "        df_list[i] = df_final\n",
    "        \n",
    "    return df_list\n",
    "        \n",
    "def get_dataset(dataframe):\n",
    "    \n",
    "    X = dataframe.drop(\"quality\", axis = 1)\n",
    "    y = dataframe[\"quality\"]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def checking_better_dataframe(lst):\n",
    "    \n",
    "    #lst = lst_of_dataframes(dataframe_1, dataframe_2)\n",
    "    \n",
    "    \"\"\" This function will get accuracy for two dataframes that we have, compare them\n",
    "    and return the dataframe wich gives us better accuracy. \"\"\"\n",
    "    \n",
    "    l_accuracy = []\n",
    "    \n",
    "    for i in range(len(lst)):\n",
    "        X, y = get_dataset(lst[i])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "        log_reg = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                       intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                       multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                       random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                       warm_start=False)\n",
    "        log_reg.fit(X_train,y_train)\n",
    "        y_hat = log_reg.predict(X_test)        \n",
    "        cm = confusion_matrix(y_hat,y_test)\n",
    "        accuracy = metrics.accuracy_score(y_hat,y_test)\n",
    "        l_accuracy.append(accuracy)\n",
    "#         print(f'Accuracy of dataframe_{i+1} is {accuracy}')\n",
    "#         print(f'log_reg.intercept_ of dataframe_{i+1} is {log_reg.intercept_}')\n",
    "#         print(f'log_reg.coef_ of dataframe_{i+1} is {log_reg.coef_}')\n",
    "#         print(f'confusion_matrix of dataframe_{i+1} is {cm}\\n')\n",
    "    if l_accuracy[0] > l_accuracy[1]:\n",
    "        \n",
    "        return lst[0]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return lst[1]\n",
    "        \n",
    "df_final = checking_better_dataframe(lst_of_dataframes(dataframe_1, dataframe_2))[[\"total_acidity\", \"chlorides\", \"pH\", \"sulphates\", \"alcohol\", \"quality\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have 2 datasets for modeling\n",
    "#### 1.1) dataframe_1 (where outliers removed, quality scaled to 2 types)\n",
    "#### 1.2) dataframe_2 (where outliers replaced with mean values, quality scaled to 2 types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic regression\n",
    "# 2. Cross-validation - KFold\n",
    "# 3. RandomForestClassifier\n",
    "# 4. SVM\n",
    "# 5. Kernel SVM\n",
    "# 6. Gaussian Kernel\n",
    "# 7. Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset transformation function\n",
    "def get_dataset(dataframe):\n",
    "\n",
    "    X = dataframe.drop(\"quality\", axis = 1)\n",
    "    y = dataframe[\"quality\"]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def log_regression(dataframe):\n",
    "    X, y = get_dataset(dataframe)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    #Logistic regression\n",
    "    log_reg = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                       intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                       multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                       random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                       warm_start=False)\n",
    "    log_reg.fit(X_train,y_train)\n",
    "    y_hat = log_reg.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_hat,y_test)\n",
    "    cm = confusion_matrix(y_hat,y_test)\n",
    "    # Let's predict target values for test dataset and check model accuracy\n",
    "    print('Logistic regression')\n",
    "    print(f'Accuracy of dataframe is: {accuracy}')\n",
    "    print(f'log_reg_intercept of dataframe is: {log_reg.intercept_}')\n",
    "    print(f'log_reg_coef of dataframe is: {log_reg.coef_}')\n",
    "    print(f'confusion_matrix of dataframe is:\\n {cm}\\n')\n",
    "\n",
    "#Cross-validation - KFold\n",
    "def k_fold_model(dataframe, k):\n",
    "    X, y = get_dataset(dataframe)\n",
    "\n",
    "    # retrieve the model to be evaluate\n",
    "    def get_model():\n",
    "        model = LogisticRegression()\n",
    "        return model\n",
    "\n",
    "    # evaluate the model using a given test condition\n",
    "    def evaluate_model(cv):\n",
    "        # get the dataset\n",
    "        #X, y = fit_dataset(dataframe)\n",
    "        # get the model\n",
    "        model = get_model()\n",
    "        # evaluate the model\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        # return scores\n",
    "        return mean(scores), scores.min(), scores.max()\n",
    "\n",
    "\n",
    "    print('Cross-validation - KFold')\n",
    "    # define folds to test\n",
    "    folds = range(2, k+1)\n",
    "\n",
    "    # record mean and min/max of each set of results\n",
    "    means, mins, maxs = list(),list(),list()\n",
    "    # evaluate each k value\n",
    "    for k in folds:\n",
    "        # define the test condition\n",
    "        cv = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "        # evaluate k value\n",
    "        k_mean, k_min, k_max = evaluate_model(cv)\n",
    "        # report performance        \n",
    "        print('> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n",
    "        # store mean accuracy\n",
    "        means.append(k_mean)\n",
    "        # store min and max relative to the mean\n",
    "        mins.append(k_mean - k_min)\n",
    "        maxs.append(k_max - k_mean)\n",
    "    # calculate the ideal test condition\n",
    "    ideal, _, _ = evaluate_model(LeaveOneOut())\n",
    "    print()\n",
    "    print('Ideal: %.3f' % ideal)\n",
    "    print()\n",
    "\n",
    "#RandomForestClassifier\n",
    "def rand_forest(dataframe):\n",
    "    X, y = get_dataset(dataframe)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    rf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                           criterion='gini', max_depth=None, max_features='auto',\n",
    "                           max_leaf_nodes=None, max_samples=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                           n_jobs=None, oob_score=False, random_state=None,\n",
    "                           verbose=0, warm_start=False)\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred=rf.predict(X_test)\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    accuracy2 = metrics.accuracy_score(y_pred, y_test)\n",
    "    cm2 = confusion_matrix(y_pred,y_test)\n",
    "    print('RandomForestClassifier')\n",
    "    print(f'Accuracy of dataframe is: {accuracy2}')\n",
    "    print(f'confusion_matrix of dataframe is:\\n {cm2}\\n')\n",
    "\n",
    "#SVM Linear, Kernel, Gaussian Kernel, Sigmoid\n",
    "def svm_func(dataframe):\n",
    "    X, y = get_dataset(dataframe)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    kernel_hyperparameter = {'SVM':'linear', 'Kernel SVM':'poly', 'Gaussian Kernel SVM':'rbf',\n",
    "                             'Sigmoid Kernel SVM':'sigmoid'}\n",
    "    for key, value in kernel_hyperparameter.items():\n",
    "        svclassifier = SVC(kernel=value)\n",
    "        svclassifier.fit(X_train, y_train)\n",
    "        y_pred = svclassifier.predict(X_test)\n",
    "        print(f'{key}')\n",
    "        print(f'Accuracy of dataframe is: {metrics.accuracy_score(y_pred, y_test)}')\n",
    "        print(f'confusion_matrix of dataframe is:\\n {confusion_matrix(y_test,y_pred)}\\n')\n",
    "    #     print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "Accuracy of dataframe is: 0.7207112970711297\n",
      "log_reg_intercept of dataframe is: [0.6087698]\n",
      "log_reg_coef of dataframe is: [[-0.16394986 -0.36181396 -0.2563822   0.44620045  0.98329446]]\n",
      "confusion_matrix of dataframe is:\n",
      " [[465 232]\n",
      " [302 913]]\n",
      "\n",
      "Cross-validation - KFold\n",
      "> folds=2, accuracy=0.708 (0.697,0.719)\n",
      "> folds=3, accuracy=0.707 (0.691,0.721)\n",
      "> folds=4, accuracy=0.707 (0.687,0.728)\n",
      "> folds=5, accuracy=0.706 (0.685,0.729)\n",
      "> folds=6, accuracy=0.707 (0.685,0.731)\n",
      "> folds=7, accuracy=0.706 (0.686,0.733)\n",
      "> folds=8, accuracy=0.707 (0.679,0.739)\n",
      "> folds=9, accuracy=0.707 (0.671,0.735)\n",
      "> folds=10, accuracy=0.706 (0.675,0.734)\n",
      "> folds=11, accuracy=0.707 (0.672,0.736)\n",
      "> folds=12, accuracy=0.706 (0.676,0.741)\n",
      "> folds=13, accuracy=0.706 (0.679,0.751)\n",
      "> folds=14, accuracy=0.706 (0.682,0.751)\n",
      "> folds=15, accuracy=0.707 (0.676,0.754)\n",
      "> folds=16, accuracy=0.706 (0.659,0.750)\n",
      "> folds=17, accuracy=0.707 (0.673,0.747)\n",
      "> folds=18, accuracy=0.707 (0.661,0.755)\n",
      "> folds=19, accuracy=0.706 (0.668,0.755)\n",
      "> folds=20, accuracy=0.706 (0.669,0.753)\n",
      "> folds=21, accuracy=0.706 (0.664,0.758)\n",
      "> folds=22, accuracy=0.706 (0.666,0.758)\n",
      "> folds=23, accuracy=0.706 (0.661,0.754)\n",
      "> folds=24, accuracy=0.707 (0.657,0.764)\n",
      "> folds=25, accuracy=0.707 (0.648,0.764)\n",
      "> folds=26, accuracy=0.706 (0.647,0.766)\n",
      "> folds=27, accuracy=0.707 (0.661,0.766)\n",
      "> folds=28, accuracy=0.707 (0.661,0.765)\n",
      "> folds=29, accuracy=0.707 (0.658,0.760)\n",
      "> folds=30, accuracy=0.707 (0.664,0.758)\n",
      "\n",
      "Ideal: 0.707\n",
      "\n",
      "RandomForestClassifier\n",
      "Accuracy of dataframe is: 0.8849372384937239\n",
      "confusion_matrix of dataframe is:\n",
      " [[ 635   88]\n",
      " [ 132 1057]]\n",
      "\n",
      "SVM\n",
      "Accuracy of dataframe is: 0.7248953974895398\n",
      "confusion_matrix of dataframe is:\n",
      " [[531 236]\n",
      " [290 855]]\n",
      "\n",
      "Kernel SVM\n",
      "Accuracy of dataframe is: 0.6752092050209205\n",
      "confusion_matrix of dataframe is:\n",
      " [[ 212  555]\n",
      " [  66 1079]]\n",
      "\n",
      "Gaussian Kernel SVM\n",
      "Accuracy of dataframe is: 0.752092050209205\n",
      "confusion_matrix of dataframe is:\n",
      " [[490 277]\n",
      " [197 948]]\n",
      "\n",
      "Sigmoid Kernel SVM\n",
      "Accuracy of dataframe is: 0.6323221757322176\n",
      "confusion_matrix of dataframe is:\n",
      " [[405 362]\n",
      " [341 804]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_regression(df_final)\n",
    "k_fold_model(df_final, 30) # k is the fold number, here k = 30\n",
    "rand_forest(df_final)\n",
    "svm_func(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We got higher accuracy score in case of RandomForest model, so we will choose this model for predicting wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the results of several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LogisticRegression: ideal=0.707, cv=0.706\n",
      ">RidgeClassifier: ideal=0.708, cv=0.707\n",
      ">KNeighborsClassifier: ideal=0.821, cv=0.815\n",
      ">DecisionTreeClassifier: ideal=0.873, cv=0.876\n",
      ">RandomForestClassifier: ideal=0.903, cv=0.898\n",
      ">LinearSVC: ideal=0.706, cv=0.706\n",
      ">SVC: ideal=0.741, cv=0.740\n",
      "Correlation: 0.999\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8fklEQVR4nO3dd5xU1f3/8dfbpa0FAUFDEUElqGgCX9cSE01igxgLlhiwoNFoEpUUI4oxiSUaC8nPxBITNYpiDyLBSiyoMaKyCNIURSywoCJFRBApn98f54xzd5idHZadndndz/PxmMfOPbfMZ+7uzmfOPeeeIzPDOeec21ibFTsA55xzjZMnEOecc3XiCcQ551ydeAJxzjlXJ55AnHPO1YknEOecc3XiCcTVSNLlkj6W9EEe274r6eAa1n1H0vz6j9A5V0yeQJoQSedIqpS0WtLILOsPkvSGpJWSJkjaIcexugO/BnYzs68UMGwU/FzSDEmfSZov6V+S9pA0XNLzWfbpKOkLSbsXKKYekkxSixrWnyppejyXH0i6SVK7jG12kzRO0ieSPo3nfL+MbVpJukTSW/G9vyvptvj6f5d0Z5bX/nr8HXfIsm5kjPuojPJrY/mpdTkfmyr+judKmlWM13eF4QmkaVkAXA7clrlCUkdgDPA7oANQCdyf41jdgcVm9lEB4sz0V+AXwM9jbF8FxgLfB+4C9pPUM2OfQcB0M5vRAPFVI+nXwNXAMGBrYF9gB+BJSa3iNjsB/wOmAz2BLsBDwH8kfSNxuNHAkcAJ8VhfByYDBwF3AMdI2iIjhJOBR8xsSQ0hvgkMScTbAjgeeLuOb7k+HABsC+woaa+GfOGavgS4emBm/mhiD0ISGZlRdibwYmJ5C2AVsEuW/Q+O69YDK1LHInzQzQSWAc8Cuyb2eRc4OD4vB0YCS4FZhA/a+TXE2gtYB+yd4/38B/h9RtkrwC+ybNslxt4hUdYP+BhoCewMPAd8Esvur+E1ewAGtMgobxvPyfEZ5VsCi4DT4vIo4LEsx70JeD7jPG+f473PBoYklssIXxSOqmH7kcCfgA+B9rHscOBx4AXg1MS2pwGvx9/TeGCHxLq/AvOA5YSEtn9i3SXAA8CdwKfxb6Kilr/J24C7CV9ibshY1wd4ElgS4/5N4r3+hpD4Po1xbJ/tdxP/Hn8cn59KSN7XAosJ/w87Ac/E5Y9jLO0S+28fY1sUt7kBaBVj2iOx3bbASqBTsf/PS+HhNZDmow/wWmrBzD4j/GP2ydzQzJ4CvgcsMLMtzexUSV8F7gV+CXQCHgMeTn3jznAx4R92J6A/cEqOuA4iJJdXcmxzB+FbNwCSegN9gXuyxL4AmAgcmyg+ARhtZmuAPxASUnugG3B9jtfNZj+gDeHDJvm6Kwjn5JBYdAjwryz7PwB8U1I5IYG8YmbzcrzenSRqE3GflvG1avI58G9CLY24f7VLYfES12+AYwi/z/8Sfr8pkwjnuAPhPP9LUpvE+iOB+4B2wDjCB25WkjYHjiN8aN8NDErU1LYCngKeICT/nYGn467nAoOBwwiJ+zTCh3c+9gHmAtsBVwACroyvsSshYVwSYygDHgHeIySnrsB9ZvZFfI8nJY47GHjazBblGUeT5gmk+diS8K076RNgqzz3/yHwqJk9GT+I/0SoaeyXZdvjgSvMbEn8cLwux3G3ARbW8toPAdsl2g+GAI/n+Ce+h/CPjiQRPkhTyWYN4XJTFzP73MxeqOW1M3UEPjaztVnWLYzrU9tle18LCf93HcjvvY8Cvi2pW1weAtwTfwe53AkMie0y3yZcEkz6KXClmb0e38sfgb6pdjEzu8vMFpvZWjP7M9Aa6J3Y/wUze8zM1sUYv54jlmOA1YTE/SghAX4/rjsc+MDM/hx/H5+a2ctx3Y+B35rZbAteM7PFtbzvlAVmdn2Mf5WZzYl/u6vj383/i+cFYG9CYhlmZp9l/F3cAQyOf0cQvsiMyjOGJs8TSPOxgvAtLqkt8Kmk/SWtiI+ZNezfhfANDQAzW0+4xNG1hm2T36rfy7JNymKgc67AzWwl4dv8kPiPfCIZ36gzPAh8Q1JnwrX39YRv2ADnE76NviJppqTTcr12Fh8DHWu4rt45rk9tl+19dY7xLCW/9/4+8DxwkqQtgYHkfu+p/V4g1CwuIrSXrMrYZAfgr5KWSVpGuFQj4u9T0nmSXo8dAJYR2mc6JvZP9sxbCbTJ0dZwCvBA/DD/nPD7SdVKt6fmtplc62pTrVYnaTtJ90mqkrSc0LaWej/bA+9l+1IQk9lK4DuSdiHUkMbVMaYmxxNI8zGTxLfE2DC7EzDTzP4bL1VtaWYbXNKKFhA+dFL7i/CPV5Vl24VxXUr3HHE9DXSTVFFL/HcQajaHEGpND9e0oZktJXzb/SHh8tV9ZuECtpl9YGZnmFkX4CfA3yTtXMtrJ00kfJs+JlkYP9y/R/ryy1PAD7LsfzwwMSbFp4C9E7WLmqQu4R0LvGNmk/OM9S5CT7psCWce8BMza5d4lJvZi5L2JyTa4wntKO0ItVVlOU5O8b0dSEiAHyh0CT8OOCx27JgH7FjD7vMIf6OZPos/N0+UZfYUzBxm/I+xbA8za0u4LJV6P/OA7jkS4B1x+5MJl0I/r2G7ZscTSBMiqUW8Tl0GlElKfit8CNhd0rFxm98D08zsjTwP/wDwfYWuwC0JH0yrgRdr2PZCSe3jB8jQmg5qZm8BfwPuVbhfpFWMe5Ck4YlN/0tovL+Z9PXpXO4hXO45jkRbiaQfJD6wlxI+VNbnOE7rGE+beN4+BS4Frpc0QFJLST3ie55P+vLGpYTeY1dI6iBpK0lDY0wXxPf+FKHx+CFJe8bf31aSfppRM3qQkIQvJXyY5es6QsLdoBs08HfC76hPPC9bS0olvK2AtYQG5RaSfs+Gtdd8nUzoFZZqt+pL6GU3n3CZ8RGgs6RfSmod3/8+cd9bgT9I6hW7AX9N0jbxElQVISmVxXOVLdEkbUWohX8iqSuhY0fKK4QvPVdJ2iL+rr+ZWH8XcDQhidRa+2tWit2K74/6exAaBS3jcUli/cHAG4SeP88CPXIc6ztk9Jwi/BPNInwbfQ7ok1j3LuleWJsT/tGWUUsvrLi9CN14ZxIuF1QRuhj3qeH97ZPHuSgn9hDKKL8mHn8F4fLImTXs3yPLubTEezwdmBHP5YfAP4i9nhLH2J3wAbk8vt6zwLcytmlFSAxzCN+s3yN8cHbP2G4k4UO9Sy3veyRweQ3rMnthnUzoZryc8C38tlheRug1tZzwwXp+xu/3EuCuLOeqRZbXfAMYmqX8fKAycZ6eJiT0D4DhiTh+C7wTf5eTgG5x3fdi+TLgz4S/x2QvrBcyXq8PoRfXCmAq4QvQ/MT67oR2olQvresy9n8qngMV+/+8lB6KJ8c551wNJN1GaJj/bbFjKSV+g41zzuUQL1EeQ7ifyCV4G4hzztVA0h8IlypHmNk7xY6n1PglLOecc3XiNRDnnHN10izaQDp27Gg9evQodhjOOdeoTJ48+WMz61TT+maRQHr06EFlZWWxw3DOuUZFUq5RJPwSlnPOubrxBOKcc65OPIE455yrE08gzjnn6sQTiHPOuTppFr2wnHOuORk7pYoR42ezYNkqurQrZ1j/3gzsl23qnk3jCcQ555qQsVOquHDMdFatWQdA1bJVXDhmOkC9JxG/hOWcc03IiPGzv0weKavWrGPE+Nn1/loFTSBxwp3ZkuZkTA6UWr+DpKclTZP0bHJmNkmnSHorPk5JlO8paXo85nWJuYqdc67ZW7Asc/bi3OWbomAJRFIZcCNh4pfdCBPT75ax2Z+AO83sa8BlwJVx3w7AxcA+hAnvL5bUPu5zE3AG0Cs+BhTqPTjnXGPTpV35RpVvikLWQPYG5pjZXAvTj94HHJWxzW7AM/H5hMT6/sCTZrbEwvzWTwIDJHUG2prZSxaGEb4TGFjA9+Ccc43KsP692XnFIt69+nDevfpwyr/4nPKWZQzr37veX6uQCaQrYZrMlPmxLOk1wkQtEKZL3UrSNjn27Rqf5zomAJLOlFQpqXLRokV1fhPOOddomDHwmvN46sYfAfB5i1Z02LY9Vx6zR5PshXUecIOkU4HnCXNVr8u5R57M7GbgZoCKigqf9MQ517S9/DLsu296+dZbaXP66fyvgC9ZyARSBWyfWO4Wy75kZguINRBJWwLHmtkySVXAdzL2fTbu3y2jvNoxnXOuWVm7Fvr2hZkzw/JXvgLvvANt2hT8pQt5CWsS0EtST0mtgEHAuOQGkjpKSsVwIXBbfD4eOFRS+9h4figw3swWAssl7Rt7Xw0B/l3A9+Ccc6Vr9Gho2TKdPJ58EhYubJDkAQWsgZjZWknnEJJBGXCbmc2UdBlQaWbjCLWMKyUZ4RLW2XHfJXEu4knxcJeZ2ZL4/CxgJFAOPB4fzjnXfHz6KbRtm14+6KCQPBr4roZmMSd6RUWF+YRSzrkmYcQIOP/89PKMGdCnT0FeStJkM6uoaX2xG9Gdc87lY/582D7RrDx0KFx3XfHiwROIc86VvtNPh9tuSy8vXBgay4vMx8JyzrlSNWVKaNdIJY8bbgCzkkge4DUQ55wrPevXw377hXs7ALbaCj74ADbfvLhxZfAaiHPOlZJHHoGysnTyeOQRWL685JIHeA3EOedKw8qV0KlT+AnwjW/ACy/AZqX7Pb90I3POuebihhtgiy3SyWPKFHjxxZJOHuA1EOecK54PPoDOndPLp58Ot95avHg2UmmnN+eca6qGDq2ePObNa1TJAzyBOOdcw5o5M3TNveGGsDxiROia261b7v1KkF/Ccs65hrB+PRx8MEyYEJY32wyWLQtddBspr4E451yhPflk6JqbSh4PPgjr1jXq5AFeA3HOucL5/HPo3h1Ss6J+/etQWQktmsZHr9dAnHOuEG69FcrL08nj5Zdh6tQmkzzAayDOOVe/Pv443BCYcsIJcNddDT5XR0PwGohzztWXCy6onjzeeQfuvrtJJg8ocAKRNEDSbElzJA3Psr67pAmSpkiaJumwWH6ipKmJx3pJfeO6Z+MxU+u2LeR7cM65Wr35ZkgS11wTli+9NHTN7dGjqGEVWsEuYUkqA24EDgHmA5MkjTOzWYnNfgs8YGY3SdoNeAzoYWZ3A3fH4+wBjDWzqYn9TjQzn2LQOVdcZnDEEfDoo+mypUuhXbuihdSQCtkGsjcwx8zmAki6DzgKSCYQA1IT+24NLMhynMHAfQWM0znnNt7zz8O3v51evvtuxu76bUb8/VUWLFtFl3blDOvfm4H9uhYvxgIrZALpCsxLLM8H9snY5hLgP5KGAlsAB2c5zg8JiSfpdknrgAeByy3LxO6SzgTOBOjevXtd4nfOuQ198QX07g3vvhuWd94ZZs1i7IyPuHDMdFatWQdA1bJVXDhmOkCTTSLFbkQfDIw0s27AYcAoSV/GJGkfYKWZzUjsc6KZ7QHsHx8nZzuwmd1sZhVmVtEp2ajlnHN1dddd0Lp1Onn897/w1lvQsiUjxs/+MnmkrFqzjhHjZzd8nA2kkDWQKiAxAzzdYlnS6cAAADObKKkN0BH4KK4fBNyb3MHMquLPTyXdQ7hUdme9R++ccylLl0KHDunlgQNhzJhqvasWLFuVddeaypuCQtZAJgG9JPWU1IqQDMZlbPM+cBCApF2BNsCiuLwZcDyJ9g9JLSR1jM9bAocDM3DOuUK55JLqyePNN+GhhzbomtulXXnW3WsqbwoKlkDMbC1wDjAeeJ3Q22qmpMskHRk3+zVwhqTXCDWNUxPtGQcA81KN8FFrYLykacBUQo3mlkK9B+dcM/bOOyFJXHppWB4+PPS66tUr6+bD+vemvGVZtbLylmUM69+70JEWjbK0Pzc5FRUVVlnpvX6dc3kwg0GD4IEH0mUffwzbbFPrrmOnVDFi/Owm0wtL0mQzq6hpvQ9l4pxzKS+9FOYiT/nnP+G00/LefWC/ro06YWwsTyDOObd2bRgpd1a8Ta1z53AJq3Xr4sZV4ordjdc554pr9Gho2TKdPJ56ChYs8OSRB6+BOOeap08/hbZt08uHHgpPPNFkBz4sBK+BOOean2uuqZ48Zs6E8eM9eWwkr4E455qPefPCDIEpv/gF/OUvRQunsfME4pxrHn70Ixg5Mr38wQew3XZFC6cpqDWBSKogjDnVBVhFuPP7STNbWuDYnHNu0736Kuy5Z3r5xhvhrLOKF08TUmMCkfQjYCjwDjAZmE0YauRbwAWSZgC/M7P3GyJQ55zbKOvWhXs6Jk0Ky1tvHXpXbb55ceNqQnLVQDYHvmlmWUcCizME9iKMZ+Wcc6Xj4YfhyCPTy48+CocdVrx4mqhcCeSVmpIHQMYMgc45V3yffRbmJF8VP7q++c0w8dNm3uG0EHKd1ZslvSXpD3G6WeecK13XXQdbbplOHlOnwgsvePIooBprIGbWT1JvwjDsoyWtIYyYe5+ZvdtA8TnnXG4LF0KXLunlM86Am28uXjzNSM7UbGazzexSM9sNGEKYt/xpSf9rkOiccy6Xs8+unjzmz/fk0YDyqtvFyZ22BbYjzF3+Ue49nHOugGbMCHeN/+1vYfnPfw7DsHdtPiPhloKc94FI2p8wb/lAYDphdsBfmdknhQ/NOecyrF8PBx4Izz0Xllu0CNPNbrllceNqpmqsgUiaB1wJzAL6mll/M7t9Y5KHpAGSZkuaI2l4lvXdJU2QNEXSNEmHxfIeklZJmhoff0/ss6ek6fGY10k+eI1zzcJ//gNlZenkMWYMrFnjyaOIctVADgUWm1m1y1WSOgGfmtnnuQ4sqQy4ETgEmA9MkjTOzGYlNvstYarbm2JPr8eAHnHd22bWN8uhbwLOAF6O2w8AHs8Vi3OuEfv8c+jWDRYvDst9+0JlZUgmrqhytYH8inDXeaZvAdfmcey9gTlmNtfMviBc/joqYxsDUkNibg0syHVASZ2Btmb2Upw7/U7C5TXnXFN0yy1QXp5OHq+8AlOmePIoEbkSyJ5mNiaz0MweAg7I49hdgXmJ5fmxLOkS4CRJ8wm1iaGJdT3jpa3nYltM6pjzazkmAJLOlFQpqXLRokV5hOucKxmLFoVG8jPPDMsnnRQayffaq7hxuWpyJZBcA8bU1505g4GRZtYNOAwYFXt8LQS6m1k/4FzgHkltcxxnA2Z2s5lVmFlFp06d6ilc51zBDRsG226bXn73XRg1qmjhuJrlSgQfSdo7s1DSXkA+X+mrgO0Ty91iWdLpwAMAZjaRMFhjRzNbbWaLY/lk4G3gq3H/brUc0znXGM2eHWodf/pTWL788lDr2GGH4sblapSrEX0Y8ICkkYTReAEqCDcUDsrj2JOAXpJ6Ej7kBwEnZGzzPnAQMFLSroQEsig21C8xs3WSdiQM2jjXzJZIWi5pX0Ij+hDg+jxicc6VKjM4/HB47LF02bJlYfRcV9JqrIGY2SvAPoCAU+NDwD5m9nJtBzaztcA5wHjgdUJvq5mSLpOUGibz18AZkl4jDJNyamwcPwCYJmkqMBr4qZktifucBdwKzCHUTLwHlnON1XPPhbGqUsnjnntCQvHk0SgofF7n2EBqA+wcF+fU1n23FFVUVFhlZWWxw3DOpXzxBey8c5hiFuCrXw13l7dsWdy4XDWSJptZRU3rc91I2ELSNYSeVHcQuszOk3SNJP8tO+fqZtQoaN06nTz+97/Q/uHJo9HJ1QYyAtgK2NHMPgWIPaH+FB+/KHx4zrkmY+lS6NAhvXzMMTB6dGg4d41Srl5YhwNnpJIHgJktB35G6HLrnHP5ufji6sljzhx48EFPHo1crhqIWZYGktgzKnfDiXPOAcydCzvtlF6+6KLQPdc1CbkSyCxJQ8zszmShpJOANwoblnOuUTOD448Pl6hSFi+uXgtxjV6uBHI2MEbSaVS/D6QcOLrQgTnnGqmJE2G//dLLt98Op55atHBc4eSa0rYK2EfSgUCfWPyYmT3dIJE55xqXtWthjz3gjXiBolu30NbRunVx43IFk8+YVpZ4rC9sOM65RumBB0I33FTyePrp0E3Xk0eTVmMNRFJXYAzwOelLWD+QdDVwdKyhOOeas+XLq981PmBAuKvce1c1C7naQG4AbjKzkclCSUOAv7Hh3B7OuebkqqvgwgvTy7Nmwa67Fi8e1+ByXcLaLTN5AMReWbsULCLnXGmbNy/UMFLJ49xzQ68rTx7NTq4aSNbkEufr8OnAnGuOTj0V7rgjvfzhh9Xn7nDNSq4ayCOSbpG0RaogPv87YfZA51xzMXlyqHWkksdNN4VahyePZi1XDeR84ErgPUnvxbLuhIEVL6xxL+dc07FuHeyzT0ggAO3bQ1VVmKfcNXu55gNZY2bnEWYVPDU+dohlVzZIdM654hk3Dlq0SCePxx+HJUs8ebgv5aqBAGBmq4DpGcXHA+cVJCLnXHF99hlssw2sXh2WDzgAJkwIEz85l1DXv4i8OnlLGiBptqQ5koZnWd9d0gRJUyRNk3RYLD9E0mRJ0+PPAxP7PBuPOTU+/CKsc/XlL3+BLbdMJ4/XXkvPGuhchlw3EtY06pnII4FIKgNuBA4B5gOTJI0zs1mJzX5LmOr2Jkm7ERrnewAfA0eY2QJJuxOmxe2a2O9EM/MpBp2rLwsXQpcu6eWf/jQ0lDuXQ65LWJMJw5dkSxZf5HHsvQlT4M4FkHQf4ebDZAIxoG18vjWwAMDMpiS2mQmUS2ptZqvzeF3n3MY466zqyaKqqnoyca4GuQZT7LmJx+5KmA43ZT6wT8Y2lwD/kTQU2AI4OMtxjgVezUget0taBzwIXJ5t3hJJZwJnAnTv3r2u78G5pmv6dPja19LL114Lv/xl0cJxjU+uOdF75NpRQbdNfP3BwEgz60aY5XBUvFEx9Rp9gKuBnyT2OdHM9gD2j4+Tsx3YzG42swozq+jUqdMmhulcE7J+fWgYTyWP1q1hxQpPHm6j5ZwTPX6Y/5twOWsR0AbYGfgucBBwMaFmkU0VoQtwSrdYlnQ6MADAzCZKagN0BD6KyekhYIiZvZ3aITWIo5l9KukewqWyO3GuGRs7pYoR42ezYNkqurQrZ1j/3gzs13XDDcePDwMefrnjWDjKh7VzdZPrEtYPYsP2icBpQGdgJfA6obH7CjP7PMexJwG9JPUkJI5BwAkZ27xPSEQjJe1KSFCLJLUDHgWGm9n/UhtLagG0M7OPJbUkzNv+1Ea8X+eanLFTqrhwzHRWrVkHQNWyVVw4JvS8/zKJrFoV5udYsiQs/9//wSuvQJmPSuTqLud9ILHH1EV1ObCZrZV0DqEHVRlwm5nNlHQZUGlm44BfA7dI+hWhQf1UM7O4387A7yX9Ph7yUOAzYHxMHmWE5HFLXeJzrqkYMX72l8kjZdWadYwYPzskkH/8I/SqSqmshD33bOAoXVNU642Em8LMHiNj3Cwz+33i+Szgm1n2uxy4vIbD+l++cwkLlq3KWr66amH1eTmGDKk+EKJzm6igCcQ5V3hd2pVTlZFEfvPMPzlz0kPpgvfeA++N6OqZ317qXCM3rH9vyluGtoydFs/j3asPTyePP/4xjJrrycMVQK01EEljgH8Cj5uZz4nuXIkZ2K8rmLHd4GP4xpuT0is++QTatq15R+c2UT41kL8Rek+9JekqSb0LHJNzbmM8+ywD99w+nTzuvz/UOjx5uALLZzTep4CnJG1NuPHvKUnzCL2f7jKzNQWO0TmXzerVsPPOMD/eirXLLjBtGrRsWdy4XLORVxuIpG0I84H8GJgC/BX4P+DJgkXmnKvZnXdCmzbp5PHii/D66548XIPKpw3kIaA3MIowQu7CuOp+ST4irnMNacmSMFdHyg9+EC5ZKa8ZFpyrV/l0473OzCZkW2FmFfUcj3OuJr/7HVyeuD3q7bdhxx2LF49r9vK5hLVbHFoEAEntJZ1VuJCcc9W8/XaoYaSSx+9+FxrJPXm4IssngZxhZstSC2a2FDijYBE55wIzOPbY0FCesmQJXHZZ8WJyLiGfBFImpS+wxpkGWxUuJOccL74YppEdMyYs33FHSCjt2xc3LucS8mkDeYLQYP6PuPyTWOacq29r1sAee8Ds2WG5e3d46y1o5d/ZXOnJpwZyATAB+Fl8PA2cX8ignGuW7r8/JIpU8pgwIYxh5cnDlah8biRcD9wUH865+vbJJ9CuXXr5sMPgkUe8a64rebXWQCT1kjRa0ixJc1OPhgjOuSbvj3+snjzeeAMefdSTh2sU8rmEdTuh9rGWMJXtncBdhQzKuSbvvfdCkrgoztd23nmhkby3DzXnGo98Eki5mT0NyMzeM7NLgO/nc3BJAyTNljRH0vAs67tLmiBpiqRpkg5LrLsw7jdbUv98j+lcyTv5ZOjRI7380UcwYkTRwnGurvJJIKslbUYYjfccSUcDW9a2U+zueyPwPWA3YHCcYz3pt8ADZtaPMGf63+K+u8XlPsAA4G+SyvI8pnOlqbIy1DruihX4m28OtY5OnYobl3N1lE833l8AmwM/B/5AuIx1Sh777Q3MMbO5AJLuA44CZiW2MSA15vTWwIL4/CjgPjNbDbwjaU48Hnkc07nSsm4d7LUXTJkSljt2hPffh/Ly4sbl3CbKWQOJ3/h/aGYrzGy+mf3IzI41s5fyOHZXYF5ieX4sS7oEOEnSfMLc6UNr2TefY6ZiP1NSpaTKRYsW5RGucwUwdiy0aJFOHk88AYsWefJwTULOBGJm64BvFfD1BwMjzawbcBgwKl4u22RmdrOZVZhZRSe/ROAa2ooV4f6No48Oy9/+dqiJ9O+fez/nGpF8LmFNkTQO+BfwWarQzMbUsl8VsH1iuVssSzqd0MaBmU2U1AboWMu+tR3TueK69lo499z08rRp4e5y55qYfL7ttwEWAwcCR8TH4XnsNwnoJamnpFaERvFxGdu8DxwEIGnX+FqL4naDJLWW1BPoBbyS5zGdK44FC0IjeSp5nH12aCT35OGaqHzuRP9RXQ5sZmslnQOMB8qA28xspqTLgEozGwf8GrhF0q8IDeqnmpkBMyU9QGgcXwucHS+nke2YdYnPuXr1k5+EXlUpCxZA587Fi8e5BqDweZ1jA+l2wod7NWZ2WqGCqm8VFRVWWemTJ7oCeO016Ns3vfzXv8LPf160cJyrT5Im55o4MJ82kEcSz9sAR5Pubutc87R+PRxwAPzvf2F5883DDYFbbFHcuJxrQPlcwnowuSzpXuCFgkXkXKl74gn43vfSy+PGwRFHFC8e54oknxpIpl7AtvUdiHMlb9Wq0K7xySdhea+9YOJEKCsrblzOFUk+o/F+Kml56gE8TJgjxLnm46abwmWqVPKYPBleecWTh2vW8rmEtVVDBOJcSfroI9huu/TyKafAyJFFC8e5UpJPDeRoSVsnlttJGljQqJwrBeeeWz15vP++Jw/nEvK5kfBiM/sktWBmy4CLCxaRc8X2+uvhhsBrrw3LV10Vbgjcfvvc+znXzOTTiJ4tydSl8d250mYWxqp68sl02SefQNu2Ne/jXDOWTw2kUtL/k7RTfPw/YHKhA3OuQT3zDGy2WTp5PPBASCiePJyrUT41iaHA74D7CXekPwmcXcignGswq1fDjjuGoUcA+vSBqVPDEOzOuZzy6YX1GeBTx7qm5/bb4bTEiDwTJ8K++xYvHucamXx6YT0pqV1iub2k8QWNyrlCWrw4NJKnksfxx4ehSTx5OLdR8mkD6Rh7XgFgZkvxO9FdY3XRRWFK2ZS334b77w8JxTm3UfJJIOsldU8tSNqBLKPzOlfS5swJSeKPfwzLF18cGsl33LG4cTnXiOXTUngR8IKk5wAB+wM/KWhUztUXMzjmmDA3ecqSJdC+fdFCcq6pyKcR/QlJ/wekLhD/Evik5j2cKxEvvAD7759eHjUKTjqpePE418TkcwkLM/sYeBRYBVwNzM9nP0kDJM2WNEfSBj25JF0raWp8vClpWSz/bqJ8qqTPU8OnSBop6Z3Eur55vVPXfKxZA716pZNHjx6hu64nD+fqVa01EEn7AicAA4EOhHtAzstjvzLgRuAQQsKZJGmcmc1KbWNmv0psPxToF8snAH1jeQdgDvCfxOGHmdno2mJwzdC998IJJ6SXn3suTPzknKt3NdZAJP1R0lvAFcA0wof7IjO7I/bEqs3ewBwzm2tmXwD3AUfl2H4wcG+W8uOAx81sZR6v6ZqrZctCI3kqeRx+eHrWQOdcQeS6hPVj4EPgJmCUmS1m43pfdQXmJZbnx7INxJ5dPYFnsqwexIaJ5QpJ0+IlsNY1HPNMSZWSKhctWrQRYbtG5/LLqzeKv/EGPPywd811rsByJZDOwOXAEcDbkkYB5ZIKMcbDIGC0ma1LFkrqDOwBJG9cvBDYBdiLcEkt6+RWZnazmVWYWUWnTp0KELIruvfeC0nid78Ly8OGhV5XvXsXNy7nmokak0H8MH8CeCJ+yz8cKAeqJD1tZifUtG9UBSTHv+4Wy7IZRPbxtY4HHjKzNYm4FsanqyXdTh7tMa4JOukkuPvu9PKiRdVvEHTOFVy+vbBWm9mDZnYcYU70J/LYbRLQS1JPSa0ISWJc5kaSdgHaAxOzHGODdpFYK0GSCA37M/J5D66JmDQp1DpSyeOWW0Ktw5OHcw1uoy9Hmdly4M48tlsr6RzC5acy4DYzmynpMqDSzFLJZBBwn5lVa1+R1INQg3ku49B3S+pEuKlxKvDTjX0PrhFauxb23BOmTQvL224bLmG1aVPcuJxrxpTxud0kVVRUWGVlZbHDcHX10EPhbvKU//wHDjmkePE410xImmxmFTWt90kPXOn69FNo1y50xwU48MAw4dNmeV15dc4VWF4JRNJ+QI/k9mZW62Us5+rsz3+G8xL9I6ZPh913L148zrkN5HMn+ihgJ0J7Q6qbrZFHO4hzG62qCrp1Sy+ffTbccEPx4nHO1SifGkgFsFtmI7dz9e6MM+DWW9PLCxfCV75SvHiccznlczF5BuD/xa5wpk4NXXNTyeP660PXXE8ezpW0fGogHYFZkl4BVqcKzezIgkXlmof16+Fb3wpzkQNsuSV8+CFsvnlx43LO5SWfBHJJoYNwzdBjj8H3v59efvjhMACic67RyGdCqcwb+Zyru5UroXNnWL48LO+7b5j4qaysuHE55zZarW0gkvaVNEnSCklfSFonaXlDBOeamBtvhC22SCePV18Nl688eTjXKOVzCesGwnAj/yL0yBoCfLWQQbkm5sMPqzeIn3Ya/POfxYvHOVcv8h1McQ5QZmbrzOx2YEBhw3JNxi9+UT15zJvnycO5JiKfGsjKOJruVEnXAAvJM/G4ZmzWLOjTJ718zTVhvg7nXJORTwI5mZAwzgF+RRgh99hCBuUaMTM49FB46qmwvNlmYbrZrbYqaljOufqXTy+s9ySVA53N7NIGiMk1Vk8/DQcfnF4ePRqO9e8azjVV+fTCOoIwDtYTcbmvpA0mhnLN2OrVoWtuKnnssQesWePJw7kmLp+2jEuAvYFlAGY2FehZsIhc43LbbWFSpw8+CMsvvRQmfWrhMwU419Tlk0DWmNknGWV5DawoaYCk2ZLmSBqeZf21kqbGx5uSliXWrUusG5co7ynp5XjM+2MDv2toH38cxq86/fSwPHhwGJpkn32KG5dzrsHkk0BmSjoBKJPUS9L1wIu17SSpDLgR+B6wGzBY0m7JbczsV2bW18z6AtcDYxKrV6XWZYy7dTVwrZntDCwFTs/jPbj6NHw4dOqUXp47F+65JyQU51yzkU8CGQr0IQykeC+wHPhlHvvtDcwxs7lm9gVwH3BUju0Hx+PXSJKAA4HRsegOYGAesbj68NZbIUlcfXVYvvTS0Ouqp1/RdK45yqcX1krgovjYGF2BeYnl+UDW6xuSdiC0qzyTKG4jqRJYC1xlZmOBbYBlZrY2ccyuNRzzTOBMgO7du29k6K4aMxg4EMYl+k4sXRqmm3XONVs1JpDaelrV83Dug4DRZrYuUbaDmVVJ2hF4RtJ0ILMtJld8NwM3A1RUVPhkWHX1wguw//7p5bvughNPLF48zrmSkasG8g1CDeJe4GVgYy9wVxFuOkzpFsuyGQScnSwws6r4c66kZ4F+wINAO0ktYi0k1zHdplizBnbZJbRvAOy0E7z+OrRsWdy4nHMlI1cbyFeA3wC7A38FDgE+NrPn8hzifRLQK/aaakVIEhvUaiTtArQHJibK2ktqHZ93BL4JzIrT6k4AjoubngL8O49Y3Ma45x5o1SqdPJ5/HubM8eThnKumxgQSB058wsxOAfYF5gDPSjonnwPHGsI5wHjgdeABM5sp6TJJyctfg4D7MuZc3xWolPQaIWFcZWaz4roLgHMlzSG0ifjIfPVl2bLQSJ66RHXkkaFrbvISlnPORar+uZ2xMtQCvk/oIdWDUIO4LXV5qbGoqKiwysrKYodR2i67DC6+OL385pvQq1fx4nHOFZ2kyWZWUdP6XI3odxIuXz0GXGpmMwoQn2tgY6dUMWL8bBYsW0WXduVcvHs5hx6xX3qD4cPhyiuLF6BzrtHI1Yh+EvAZ8Avg50rfJCbAzKxtgWNz9WzslCouHDOdVWvWgRkX3Hkph77+fHqDRYugY8fiBeica1RytYFsZmZbxUfbxGMrTx6N04jxs1m1Zh3933yRd685giNj8rjymF+Hez08eTjnNoKPeNeMfLT4U97908BqZb1/PYYvWrTiwuKE5JxrxHxmwebi8st5K5E8LhgwlB4XPMLqFq3o0q68eHE55xotr4E0dYsXb3Bpquf54zCF7w7lLcsY1r93MSJzzjVyXgNpyo4+unryePZZxr46ny7tt0BA13blXHnMHgzsl3U4Meecy8lrIE3RrFnQp096+atfhdmzgTB0sScM51x98ATS1Gy7beiOmzJnThjHyjnn6plfwmoqHn88DEOSSh5DhoSuuZ48nHMF4jWQxm7dug3nH1+2DLbeuijhOOeaD6+BNGbXXls9efzlL6HW4cnDOdcAvAbSGH3yyYazAa5dC2VlRQnHOdc8eQ2ksRkypHryePzxUOvw5OGca2BeA2ks3n4bdt45vfyVr8DChcWLxznX7HkNpDHo1at68pg1y5OHc67oCppAJA2QNFvSHEnDs6y/VtLU+HhT0rJY3lfSREkzJU2T9MPEPiMlvZPYr28h30NRTZgQuubOmROWjzkmXK7addfixuWccxTwEpakMuBGwlzq84FJksYlpqbFzH6V2H4o0C8urgSGmNlbkroAkyWNN7Nlcf0wMxtdqNiLbv36Dds0Pv4YttmmOPE451wWhayB7A3MMbO5ZvYFcB9wVI7tBwP3ApjZm2b2Vny+APgI6FTAWEvHP/5RPXlccUWodXjycM6VmEI2oncF5iWW5wP7ZNtQ0g5AT+CZLOv2BloBbyeKr5D0e+BpYLiZrc6y35nAmQDdu3ev41toQCtWwFZbVS/74gto2bI48TjnXC1KpRF9EDDazNYlCyV1BkYBPzKz9bH4QmAXYC+gA3BBtgOa2c1mVmFmFZ06lXjl5ayzqiePhx4KtQ5PHs65ElbIGkgVsH1iuVssy2YQcHayQFJb4FHgIjN7KVVuZqnuR6sl3Q6cV28RN7T334cddkgvl5fDypXFi8c55zZCIWsgk4BeknpKakVIEuMyN5K0C9AemJgoawU8BNyZ2VgeayVIEmF08hmFegMFteee1ZPH1KmePJxzjUrBEoiZrQXOAcYDrwMPmNlMSZdJOjKx6SDgPjOzRNnxwAHAqVm6694taTowHegIXF6o91AQEyeGrrmvvhqWDzkkXK76+teLG5dzzm0kVf/cbpoqKiqssrKyuEGYwWYZ+fqDD2C77YoTj3PO1ULSZDOrqGl9qTSiN22jRlVPHr/5TUgonjycc42Yj4VVSKtWweabb1jWpk1x4nHOuXrkNZBCOf/86snjnntCrcOTh3OuifAaSH1buBC6dKletn59aDh3zrkmxGsg9em7362ePF5+OdQ6PHk455ogr4HUh1dfDfd1pOy7b+iu65xzTZgnkE1hFuYkX78+XTZvHnTrVryYnHOugfglrLp68MHQNTeVPH7+85BQPHk455oJr4HUYuyUKkaMn82CZavo0q6c8w/syVH77Fh9oxUrYIstihOgc84ViddAchg7pYoLx0ynatkqDDj20duqJ49bbw21Dk8ezrlmyGsgOVz68ExWrVlH+5WfMOX6E6uvXLduw6FJnHOuGfEEUoPfjp3O0pVruOXBP3DInJe/LD/+hKuYtP3uvOPJwznXzHkCyWLslCruful9Hr395/T5aC4Ab22zPYf8+CYAurYrL2Z4zjlXEvxrdBYjxs/GgOv3+yEA+//k1i+TB8Cw/r2LFJlzzpUOr4FksWDZKgCe6P1NelzwSLV17cpbMrBf12KE5ZxzJcVrIFl0qeESlYBLjuzTsME451yJKmgCkTRA0mxJcyQNz7L+2sSMg29KWpZYd4qkt+LjlET5npKmx2NeF6e2rVfD+vemvGVZ9ViBE/ft7rUP55yLCnYJS1IZcCNwCDAfmCRpnJnNSm1jZr9KbD8U6BefdwAuBioAAybHfZcCNwFnAC8DjwEDgMfrM/ZUkkjeQDisf29PHs45l1DINpC9gTlmNhdA0n3AUcCsGrYfTEgaAP2BJ81sSdz3SWCApGeBtmb2Uiy/ExhIPScQCEnEE4ZzztWskJewugLzEsvzY9kGJO0A9ASeqWXfrvF5rcd0zjlXWKXSiD4IGG1m6+rrgJLOlFQpqXLRokX1dVjnnHNRIRNIFbB9YrlbLMtmEHBvHvtWxee1HtPMbjazCjOr6NSp00aG7pxzrjaFTCCTgF6SekpqRUgS4zI3krQL0B5IzsA0HjhUUntJ7YFDgfFmthBYLmnf2PtqCPDvAr4H55xzNShYI7qZrZV0DiEZlAG3mdlMSZcBlWaWSiaDgPvMzBL7LpH0B0ISArgs1aAOnAWMBMoJjef13oDunHOudkp8bjdZkhYB72VZ1RH4uIHD2RilHF8pxwalHV8pxwalHV8pxwZNL74dzKzGNoBmkUBqIqnSzCqKHUdNSjm+Uo4NSju+Uo4NSju+Uo4Nml98pdILyznnXCPjCcQ551ydNPcEcnOxA6hFKcdXyrFBacdXyrFBacdXyrFBM4uvWbeBOOecq7vmXgNxzjlXR55AnHPO1UmTSSClPvdIXeOT1FfSREkzJU2T9MPEPiMlvZPYr29DxxfXrUusG5co7ynp5XjM++OIBA0Wm6TvJsqnSvpc0sC4riHPXXdJEyRNib/DwxLrLoz7zZbUP99jFjo2SYdImhz//idLOjCxz7PxmKlzt20R4ushaVUihr8n9qmX/9tNiO3EjL+79am/rwY+dztIejrG9qykbol19fOZZ2aN/kG40/1tYEegFfAasFuO7YcS7owH6ADMjT/bx+ft47pXgH0J80k9DnyvCPF9FegVn3cBFgLt4vJI4Lhinr+4vKKG7R4ABsXnfwd+1tCxJco7AEuAzRv63BEaLn8Wn+8GvJt4/hrQmjAa9dvxeBv1ngsUWz+gS3y+O1CV2OdZoKLI564HMKOG427y/+2mxJaxzR7A20U6d/8CTonPDwRGJf4X6uUzr6nUQL6ce8TMvgBSc4/UZDDpwRu/nHvEwoRVqblHOhPnHrFwZlNzjzRofGb2ppm9FZ8vAD4C6nt0yE05f1nFby4HAqNj0R3U7fzVV2zHAY+b2co6xLCp8RnQNj7fGlgQnx9FGMZntZm9A8yJx9vY91zvsZnZlPj3BjATKJfUug4xFCS+mtTj/219xTY47lvf8olvN9JTZExIrK+3z7ymkkBKfe6RTYkvuW5vwreNtxPFV8Qq6rWb8A++qfG1URg6/6XUJSJgG2CZma2t7ZgFji0lc8RnaLhzdwlwkqT5hFk0h9ayb97vuYCxJR0LvGpmqxNlt8dLML+r6yWieoivZ7x89Jyk/RPHrI//2/o6dz9kw7+7hjp3rwHHxOdHA1tJ2ibHvht97ppKAtkY9T73SD3LGl/8djAK+JGZrY/FFwK7AHsRqqMXFCm+HSwMj3AC8BdJOzVAHNnkOnd7EAb2TGnIczcYGGlm3YDDgFGSSuV/L2dskvoAVwM/SexzopntAewfHycXIb6FQHcz6wecC9wjqW2O4zRkbABI2gdYaWYzEvs05Lk7D/i2pCnAtwlTX9Tr516p/BFvqqLOPVLg+Ij/GI8CF1mczhfAzBZasBq4nVCtbfD4zKwq/pxLuMbbD1gMtJOUGvG5rudvk2KLjgceMrM1iZgb8tydTmgPwswmAm0Ig9rl+tvL9z0XKjZio+tDwBAz+7LWm/h9fwrcQxHOXbzstziWTybUyr9K/f3fbtK5i3L9rxT83JnZAjM7JibZi2LZshz7bvy529TGnFJ4EIaln0u4fJFqUOqTZbtdgHeJN1BaukHpHUJjUvv4vINlb1A6rAjxtQKeBn6ZZfvO8aeAvwBXFSG+9kDr+Lwj8BaxMY/QiJdsRD+rIWNLrHsJ+G6xzl382zk1Pt+VcK1cQB+qN6LPJTSO5vWeCxxbu7j9MVmO2TE+b0lo4/ppEc5dJ6Aslu9I+KCrt//bTYktLm8WY9qxiOeuI7BZfH4FYVoMqMfPvI0OvFQfhCrkm4RvIhfFssuAIxPbXEKWDwrgNEID5hzCJaJUeQUwIx7zBrJ8OBU6PuAkYA0wNfHoG9c9A0yPMd4FbFmE+PaLMbwWf56eWLdj/IOcQ0gmrYvwu+0R/5E3yyhvsHNHaMz8XzxHU4FDE/teFPebTaLHS7ZjNmRswG+BzzL+7rYFtgAmA9MIjet/JX6QN3B8x8bXnwq8ChxR3/+3m/h7/Q7wUsbxGvrcHUf4QvcmcCuJ/z/q6TPPhzJxzjlXJ02lDcQ551wD8wTinHOuTjyBOOecqxNPIM455+rEE4hzzrk68QTiGpQkk3RXYrmFpEWSHmmA10691lWFfq1CkTRa0o7x+RWS5klakbFNa4XRj+cojIbco4Zj7RKH1JiSa/SAzOMnykdKOq6GdedJeiMef5KkIZIulnRlxnZ9Jb0enz8lqX3OE+BKiicQ19A+A3aXVB6XD6Hud/hvrEMIfeJ/sAljENUqcfd9fR+3D+G+gbmx6GGy38l8OrDUzHYGriUMRZLNQMLQL/0scad5PcT5U8K53tvM+gIHEW5Mu5cwNlRS8m7tUcBZ9RWHKzxPIK4YHgO+H59XGz1X0haSbpP0SvxmfFQs7yHpv5JejY/9Yvl34lwHo+M33rtzJIfBhJu33ge+kXjNAfGYr0l6OpZtKen2ODfCNEnHxvIVif2OkzQyPh8p6e+SXgaukbS3wjwuUyS9KKl33K5M0p8kzYjHHSrpQEljE8c9RNJDWeI/Efh3asHCqKkLs2x3FGH0Ywh3Ox+UeU4U5q74JfAzSRNi2bkxrhmSfpl5UAU3KMxB8RThxsJsfkMY5nx5jHO5md1hZm8CS+MYUSnHk/79jyP8jlxjUde7IP3hj7o8gBXA1wgfbG0Id/B+B3gkrv8jcFJ83o5QY9gC2BxoE8t7AZXx+XeATwjj9mwGTAS+leV12xCGmigHzgSuj+WdCCOT9ozLqSEdrgb+ktg/NV/CikTZcYTB9CDML/II6eE12gIt4vODgQfj85/F955a14Hw7fwNoFMsu4fEndWJ13sO2CPbOc1YngF0Syy/TRxCI2O7S4Dz4vM9CXfmbwFsSbhTul/y+ISRXZ8kDLfSBVhGxpwq8X0vzfH7Pw+4Nj7fN/V7TKx/C9im2H+n/sjv4TUQ1+DMbBphiJHBhNpI0qHAcElTCQMztgG6E8YOukXSdMKwKLsl9nnFzOZbGKV4ajx2psOBCWa2CngQGCipjPAh9ryF+TgwsyVx+4OBGxMxL83jrf3L0iMBbw38S9IMwmWkPonj/sPiMPcW5mQwwuWbkyS1I9SOHs9y/M7AojziqItvEQac/MzMVgBjCKPFJh0A3Gtm6yzMFZJt2Pza3A8cF0etzTb45UeE5OQagYJcq3UuD+OAPxFqENskygUca2azkxtLugT4EPg6oabxeWJ1cp6KdWT/ux4MfEvSu3F5G8KEVxsrOfZPm4x1nyWe/4GQsI6OjdjP1nLc2wltGp8TEtHaLNusyvKa2aRGW50f22O2BhZLup0wUvICMzss1wHqysyWS1ohaUdLt9Uk18+T9A5hePFjSVxKjNoQ3qdrBLwG4orlNuBSM5ueUT4eGJq6Zi+pXyzfGlgYaxknEy6j5CUOh78/Yf6IHmbWAzibkFReAg6Q1DNu2yHu9mTcJnWMVO+gDyXtGr9BH53jZbcm3Tng1ET5k8BPUg3tqdeL3+gXEAYxvL2GY74O7Fzb+yUk51Pi8+OAZyz4kZn1rSF5/JdQK9tc0haE9/bfjG2eB34Y23E6A9+t4fWvBG6M5z3VnjQksf5eQq1srpl9OYFR/J1/hTCqsmsEPIG4ooiXnK7LsuoPhMtV0yTNjMsAfwNOkfQaYej2z7LsW5OjCR+iyZrKv4EjgOWENpEx8dj3x/WXA+1jg/JrpD8shxPaOl4kTGpUk2uAKxUm80nWiG4lNOJPi8c9IbHubmCemb1ewzEfJdTYAJB0jcJseJtLmh9raQD/BLaRNIcw2dLwHHECYGavEtpxXgFeBm41sykZmz1EaKOYRZjudGINh7uJMIXqpHgJ77/A+sT6fxEu6WVevtqTMIJtttqXK0E+Gq9zJULSDcAUM/tnDevLCR/M37TSnVGzziT9FRhnZk8XOxaXH6+BOFcCJE0m9E67q6ZtYgeAi6nbHN+NwQxPHo2L10Ccc87ViddAnHPO1YknEOecc3XiCcQ551ydeAJxzjlXJ55AnHPO1cn/B6hD9CTXxtS8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the dataset\n",
    "def get_dataset(df_final):\n",
    "    X = df_final.drop(\"quality\", axis = 1)\n",
    "    y = df_final[\"quality\"]\n",
    "    norm = StandardScaler().fit(X)\n",
    "    X = pd.DataFrame(columns = X.columns, data = norm.transform(X))\n",
    "    return X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(LogisticRegression())\n",
    "    models.append(RidgeClassifier())\n",
    "    models.append(KNeighborsClassifier())\n",
    "    models.append(DecisionTreeClassifier())\n",
    "    models.append(RandomForestClassifier())\n",
    "    models.append(LinearSVC())\n",
    "    models.append(SVC())        \n",
    "#     models.append(GaussianNB())\n",
    "#     models.append(ExtraTreesClassifier())\n",
    "#     models.append(BaggingClassifier())\n",
    "#     models.append(GaussianProcessClassifier())\n",
    "#     models.append(GradientBoostingClassifier())\n",
    "    return models\n",
    " \n",
    "# evaluate the model using a given test condition\n",
    "def evaluate_model(cv, model):\n",
    "    # get the dataset\n",
    "    X, y = get_dataset(df_final)\n",
    "    # evaluate the model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # return scores\n",
    "    return mean(scores)\n",
    " \n",
    "# define test conditions\n",
    "ideal_cv = LeaveOneOut()\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# get the list of models to consider\n",
    "models = get_models()\n",
    "# collect results\n",
    "ideal_results, cv_results = list(), list()\n",
    "# evaluate each model\n",
    "for model in models:\n",
    "    # evaluate model using each test condition\n",
    "    cv_mean = evaluate_model(cv, model)\n",
    "    ideal_mean = evaluate_model(ideal_cv, model)\n",
    "    # check for invalid results\n",
    "    if isnan(cv_mean) or isnan(ideal_mean):\n",
    "        continue\n",
    "    # store results\n",
    "    cv_results.append(cv_mean)\n",
    "    ideal_results.append(ideal_mean)\n",
    "    # summarize progress\n",
    "    print('>%s: ideal=%.3f, cv=%.3f' % (type(model).__name__, ideal_mean, cv_mean))\n",
    "# calculate the correlation between each test condition\n",
    "corr, _ = pearsonr(cv_results, ideal_results)\n",
    "print('Correlation: %.3f' % corr)\n",
    "# scatter plot of results\n",
    "plt.scatter(cv_results, ideal_results)\n",
    "# plot the line of best fit\n",
    "coeff, bias = polyfit(cv_results, ideal_results, 1)\n",
    "line = coeff * asarray(cv_results) + bias\n",
    "plt.plot(cv_results, line, color='r')\n",
    "# label the plot\n",
    "plt.title('10-fold CV vs LOOCV Mean Accuracy')\n",
    "plt.xlabel('Mean Accuracy (10-fold CV)')\n",
    "plt.ylabel('Mean Accuracy (LOOCV)')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we also got higher accuracy score in case of RandomForest model, so we will choose this model for predicting wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataframe):\n",
    "\n",
    "    X = dataframe.drop(\"quality\", axis = 1)\n",
    "    y = dataframe[\"quality\"]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = get_dataset(df_final)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "rf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "rf.fit(X_train,y_train)\n",
    "# save the model to disk\n",
    "filename = 'instance.json'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model is Cross-validation-KFold for Random Forest Classifier model, but our computers need more time for   it. So we just write a code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation - KFold\n",
    "def k_fold_model(dataframe):\n",
    "    X, y = get_dataset(dataframe)\n",
    "    # retrieve the model to be evaluate\n",
    "    def get_model():\n",
    "        model = RandomForestClassifier()\n",
    "        return model\n",
    "    # evaluate the model using a given test condition\n",
    "    def evaluate_model(cv):\n",
    "        # get the dataset\n",
    "        #X, y = fit_dataset(dataframe)\n",
    "        # get the model\n",
    "        model = get_model()\n",
    "        # evaluate the model\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        # return scores\n",
    "        return mean(scores), scores.min(), scores.max()\n",
    "    print('Cross-validation - KFold-RandomForestClassifier')\n",
    "    # define folds to test\n",
    "    folds = range(2, 11)\n",
    "    # record mean and min/max of each set of results\n",
    "    means, mins, maxs = list(),list(),list()\n",
    "    # evaluate each k value\n",
    "    for k in folds:\n",
    "        # define the test condition\n",
    "        cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "        # evaluate k value\n",
    "        k_mean, k_min, k_max = evaluate_model(cv)\n",
    "        # report performance\n",
    "        # print('> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n",
    "        # store mean accuracy\n",
    "        means.append(k_mean)\n",
    "        # store min and max relative to the mean\n",
    "        mins.append(k_mean - k_min)\n",
    "        maxs.append(k_max - k_mean)\n",
    "    # calculate the ideal test condition\n",
    "    ideal, _, _ = evaluate_model(LeaveOneOut())\n",
    "    print()\n",
    "    print('Ideal: %.3f' % ideal)\n",
    "    print()\n",
    "    # save the model to disk\n",
    "    filename = 'instance.json'\n",
    "    pickle.dump(get_model(), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation - KFold-RandomForestClassifier\n",
      "\n",
      "Ideal: 0.901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_fold_model(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
